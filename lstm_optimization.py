# -*- coding: utf-8 -*-
"""LSTM optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uqDuMYctkjfnRc1UQOqX9nE0OJ8oz4Zo
"""

# ============================================================
# Advanced Time Series Forecasting with LSTM
# Hyperparameter Optimization + Explainability
# ============================================================

# -----------------------------
# INSTALL (Colab only)
# -----------------------------
!pip install optuna==3.5.0 --no-cache-dir

# -----------------------------
# IMPORTS
# -----------------------------
import optuna
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.stattools import adfuller

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

print("Optuna version:", optuna.__version__)

# -----------------------------
# 1️⃣ DATA GENERATION
# -----------------------------
np.random.seed(42)

n_steps = 500
time = np.arange(n_steps)

series_1 = 0.05*time + 10*np.sin(2*np.pi*time/50) + np.random.normal(0, 1, n_steps)
series_2 = 0.03*time + 8*np.cos(2*np.pi*time/30) + np.random.normal(0, 1, n_steps)
series_3 = 0.02*time + 6*np.sin(2*np.pi*time/20) + np.random.normal(0, 1, n_steps)

data = np.vstack([series_1, series_2, series_3]).T
df = pd.DataFrame(data, columns=["feature_1", "feature_2", "feature_3"])

# -----------------------------
# 2️⃣ STATIONARITY CHECK
# -----------------------------
print("\nADF Stationarity Test (p-values)")
for col in df.columns:
    p = adfuller(df[col])[1]
    print(f"{col}: {p:.4f}")

# -----------------------------
# 3️⃣ SCALING
# -----------------------------
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df.values)

# -----------------------------
# 4️⃣ SEQUENCE CREATION
# -----------------------------
def create_sequences(data, lookback=30, horizon=2):
    X, y = [], []
    for i in range(len(data) - lookback - horizon):
        X.append(data[i:i+lookback])
        y.append(data[i+lookback:i+lookback+horizon, 0])
    return np.array(X), np.array(y)

LOOKBACK = 30
HORIZON = 2

X, y = create_sequences(scaled_data, LOOKBACK, HORIZON)

split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# -----------------------------
# 5️⃣ MODEL BUILDER (OPTUNA)
# -----------------------------
def build_model(trial):
    model = Sequential()

    layers = trial.suggest_int("layers", 1, 3)
    units = trial.suggest_int("units", 32, 128)
    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)

    for i in range(layers):
        model.add(LSTM(units, return_sequences=(i < layers - 1)))
        model.add(Dropout(dropout))

    model.add(Dense(HORIZON))
    model.compile(optimizer=Adam(lr), loss="mse")
    return model

# -----------------------------
# 6️⃣ OPTUNA OPTIMIZATION
# -----------------------------
def objective(trial):
    model = build_model(trial)
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)
    preds = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test.flatten(), preds.flatten()))
    return rmse

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=15)

print("\nBest Hyperparameters:")
print(study.best_params)

# -----------------------------
# 7️⃣ FINAL MODEL TRAINING
# -----------------------------
final_model = build_model(optuna.trial.FixedTrial(study.best_params))
final_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

# -----------------------------
# 8️⃣ EVALUATION (RMSE + MASE)
# -----------------------------
preds = final_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test.flatten(), preds.flatten()))

naive = y_test[:-1]
mase = np.mean(np.abs(y_test[1:] - preds[:-1])) / np.mean(np.abs(y_test[1:] - naive))

print("\nEvaluation Metrics")
print("RMSE:", rmse)
print("MASE:", mase)

# -----------------------------
# 9️⃣ EXPLAINABILITY — INTEGRATED GRADIENTS (EAGER MODE)
# -----------------------------
def integrated_gradients(model, baseline, inputs, steps=50):
    baseline = tf.cast(baseline, inputs.dtype)
    inputs = tf.cast(inputs, tf.float32)

    scaled_inputs = [
        baseline + (float(i) / steps) * (inputs - baseline)
        for i in range(steps + 1)
    ]

    grads = []
    for x in scaled_inputs:
        with tf.GradientTape() as tape:
            tape.watch(x)
            preds = model(x)
        grad = tape.gradient(preds, x)
        grads.append(grad)

    avg_grads = tf.reduce_mean(tf.stack(grads), axis=0)
    return (inputs - baseline) * avg_grads

# -----------------------------
# APPLY INTEGRATED GRADIENTS
# -----------------------------
sample = tf.convert_to_tensor(X_test[:1], dtype=tf.float32)
baseline = tf.zeros_like(sample)

ig = integrated_gradients(final_model, baseline, sample)

# Average over time dimension
importance = tf.reduce_mean(tf.abs(ig), axis=1).numpy()

# -----------------------------
# FEATURE IMPORTANCE PLOT
# -----------------------------
plt.figure(figsize=(6,4))
plt.bar(["feature_1", "feature_2", "feature_3"], importance[0])
plt.title("Feature Importance using Integrated Gradients")
plt.ylabel("Attribution Strength")
plt.show()